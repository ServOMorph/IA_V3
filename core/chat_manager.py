from core.ollama_client import OllamaClient
from core.sav_manager import SaveManager
from core.commands import CommandHandler
from config import (
    DEFAULT_MODEL,
    SAVE_DIR,
    PRESET_MESSAGES,
    WELCOME_MESSAGE,
    EMPTY_PROMPT_WARNING,
    DEFAULT_SYSTEM_PROMPT,  # ðŸ“Œ Ajout du prompt systÃ¨me
)

class ChatManager:
    def __init__(self, model=DEFAULT_MODEL, save_dir=SAVE_DIR):
        self.save_manager = SaveManager(save_dir=save_dir)
        self.client = OllamaClient(model=model, session_file=self.save_manager.session_file)
        self.commands = CommandHandler(self)

        # ðŸ“Œ CrÃ©ation immÃ©diate du fichier de sauvegarde vide s'il n'existe pas encore
        if not self.save_manager.session_file.exists():
            self.save_manager.session_file.write_text("", encoding="utf-8")

        # ðŸ“Œ Ajout du prompt systÃ¨me si c'est une nouvelle session
        if not self.client.history:  # Pas encore d'historique â†’ premiÃ¨re utilisation
            self.client.history.append({"role": "system", "content": DEFAULT_SYSTEM_PROMPT})

    def start_chat(self):
        print(WELCOME_MESSAGE)

        while True:
            user_prompt = input("\nðŸ’¬ Vous : ").strip()

            # EmpÃªcher les prompts vides tÃ´t
            if not user_prompt:
                print(EMPTY_PROMPT_WARNING)
                continue

            # DÃ©lÃ©gation des commandes
            if self.commands.is_command(user_prompt):
                handled, should_exit = self.commands.handle(user_prompt)
                if should_exit:
                    break
                if handled:
                    continue  # ne pas envoyer Ã  l'IA si la commande a Ã©tÃ© traitÃ©e

            # RequÃªte classique â†’ envoi Ã  l'IA
            answer = self.client.send_prompt(user_prompt)
            print(f"ðŸ¤– Ollama : {answer}")

            # Sauvegarde automatique aprÃ¨s chaque rÃ©ponse
            self.save_manager.save_txt(self.client.history)
