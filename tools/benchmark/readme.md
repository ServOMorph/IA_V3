# ğŸ“Š Module Benchmark IA

Ce module permet d'exÃ©cuter des benchmarks sur diffÃ©rents modÃ¨les IA (via Ollama) afin de mesurer :

* le **temps de rÃ©ponse moyen**,
* la **qualitÃ© du code gÃ©nÃ©rÃ©** (via des exercices et des tests unitaires automatiques),
* la **complÃ©tude fonctionnelle** (par ex. boucles de jeu, inventaire, systÃ¨me de combat pour l'exercice RPG).

## ğŸ“‚ Structure du dossier

```
tools/benchmark/
 â”œâ”€â”€ __init__.py
 â”œâ”€â”€ cli.py                 # Interface CLI principale (point d'entrÃ©e)
 â”œâ”€â”€ runner.py              # Logique de benchmark (exÃ©cution des tests)
 â”œâ”€â”€ exercises.py           # DÃ©finition des exercices (prompt + tests attendus)
 â”œâ”€â”€ code_utils.py          # Extraction et Ã©valuation du code gÃ©nÃ©rÃ©
 â”œâ”€â”€ storage.py             # Sauvegarde du code et des rÃ©sultats
 â”œâ”€â”€ config_benchmark.py    # Configuration spÃ©cifique au benchmark
 â”œâ”€â”€ benchmark_mistral.py   # Script dÃ©diÃ© Mistral (mistral:latest)
 â”œâ”€â”€ benchmark_deepseek.py  # Script dÃ©diÃ© DeepSeek (deepseek-tests)
```

Un alias pratique est disponible Ã  la racine :

```
main_benchmark.py  # Point d'entrÃ©e depuis la racine du projet
```

## ğŸš€ Utilisation

### Lancer le benchmark interactif

Depuis la racine du projet :

```bash
python main_benchmark.py
```

ou :

```bash
python -m tools.benchmark
```

### Lancer un benchmark direct (scripts dÃ©diÃ©s)

* **Mistral standard** :

```bash
python tools/benchmark/benchmark_mistral.py
```

* **Mistral optimisÃ© (mistral-tests)** :

```bash
python tools/benchmark/benchmark_mistral2.py
```

* **DeepSeek optimisÃ© (deepseek-tests)** :

```bash
python tools/benchmark/benchmark_deepseek.py
```

### Ã‰tapes interactives

1. **SÃ©lection de l'exercice** (par ex. `is_prime`, `fibonacci`, `factorial`, `long_code_test`).
2. **Mode test Ã  blanc** : permet de vÃ©rifier le prompt et les chemins de sauvegarde sans exÃ©cuter.
3. **SÃ©lection du mode de test** :

   * `o` â†’ exÃ©cute sur **tous les modÃ¨les installÃ©s**,
   * `n` â†’ permet de choisir un modÃ¨le spÃ©cifique.
4. **Nombre de runs** : nombre dâ€™essais pour calculer les moyennes.

### RÃ©sultats produits

* **Code gÃ©nÃ©rÃ©** : sauvegardÃ© dans

  ```
  data/generated/<model>/<exercise>/solution_<model>_<exercise>_runX_<timestamp>.py
  ```
* **Logs texte** : `data/dev_responses.log`
* **Logs JSONL** : `data/dev_responses.jsonl` (facilement exploitable pour analyse).
* **Rapport PDF** (optionnel) : synthÃ¨se graphique et qualitative des performances.

## âš™ï¸ Exercices disponibles

DÃ©finis dans `exercises.py` :

* `is_prime(n: int) -> bool`
* `fibonacci(n: int) -> int`
* `factorial(n: int) -> int`
* `long_code_test` â†’ programme plus complexe (\~200 lignes, ex. RPG console)

Chaque exercice contient :

* un **prompt** donnÃ© au modÃ¨le,
* une **suite de tests** validant la fonction gÃ©nÃ©rÃ©e.

## ğŸ“ˆ Analyse des rÃ©sultats

Un script dâ€™analyse peut agrÃ©ger les rÃ©sultats JSONL :

```bash
python tools/analyze_results.py
```

Il affiche un tableau rÃ©capitulatif (temps moyen, score moyen, taux de complÃ©tion) par modÃ¨le et par exercice.

Les codes longs (RPG) peuvent Ãªtre Ã©valuÃ©s aussi sur :

* **complÃ©tude fonctionnelle** (combat, inventaire, boucle de jeu),
* **robustesse du code** (absence de bugs bloquants),
* **taille effective** (proche de la cible demandÃ©e).

## âœ… Exemple de session (CLI standard)

```
=== SÃ©lection de l'exercice ===
1. is_prime
2. fibonacci
3. factorial
4. long_code_test
SÃ©lectionnez un exercice (1-4) [1] : 4
```

## ğŸ“š Documentation associÃ©e

* `docs/prompt_engineering_mistral7b.md` â†’ bonnes pratiques de prompt pour Mistral.
* `docs/prompt_engineering_deepseek.md` â†’ bonnes pratiques de prompt pour DeepSeek.
* `docs/comparatif_mistral_vs_deepseek.md` â†’ comparatif de performance.
* `docs/ollama_models_readme.md` â†’ explication des paramÃ¨tres Ollama.
* `docs/analyse_rpg_benchmark.pdf` â†’ exemple de rapport dâ€™analyse avec graphiques.

## ğŸ”® AmÃ©liorations prÃ©vues

* Extraction plus robuste du code gÃ©nÃ©rÃ© (ignorer les phrases hors bloc de code).
* Ajout de nouveaux exercices (par ex. `palindrome`, `tri rapide`, `simulateur`).
* Export automatique des rÃ©sultats en CSV/Excel et PDF.
* Dashboard de visualisation (Grafana/Streamlit).
* IntÃ©gration d'une Ã©valuation qualitative (complÃ©tude fonctionnelle, jouabilitÃ©).
