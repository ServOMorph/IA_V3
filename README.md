# IA\_V3 â€“ Chat IA avec Ollama, gestion avancÃ©e des conversations et interface Kivy

## ğŸ“Œ Description

Projet backend + UI pour interagir avec un modÃ¨le IA local via **Ollama** (par dÃ©faut `mistral`), avec gestion avancÃ©e des conversations et interface Kivy.

### FonctionnalitÃ©s principales :

* Dialogue avec un modÃ¨le IA local.
* Sauvegarde des conversations dans un **dossier par session** (`/sav/<nom_session>/conversation.md`).
* Sauvegarde automatique des blocs de code Python extraits en `.py`.
* Chargement, renommage, suppression et organisation des sessions.
* Copie rapide des derniers messages dans le presse-papier (CLI).
* Conservation du contexte conversationnel.
* Logs techniques et conversationnels sÃ©parÃ©s (`/logs/<nom_session>.log`).
* **Interface Kivy moderne** avec zones distinctes : liste de conversations, chat, saisie, panneau info/config.
* Couleurs centralisÃ©es dans `ui/config_ui.py`.
* Mode **DEV** :

  * Liste les modÃ¨les Ollama installÃ©s au dÃ©marrage.
  * Permet de choisir le modÃ¨le IA Ã  utiliser.
  * Affiche et logge le temps de rÃ©ponse pour chaque prompt.
  * Sauvegarde chaque essai et benchmark dans `data/dev_responses.log` et `data/dev_responses.jsonl`.

---

## ğŸš€ NouveautÃ©s et changements rÃ©cents

### Nouvelles fonctionnalitÃ©s CLI :

* Ajout de la commande `&run` : exÃ©cute le dernier script Python sauvegardÃ© de la conversation en cours.
  â†’ Un nouveau terminal Windows (`cmd.exe`) s'ouvre et lance le script (`python <fichier>`).
* AmÃ©lioration de la saisie utilisateur en mode CLI :
  â†’ Une ligne simple : taper EntrÃ©e envoie directement.
  â†’ Texte multi-lignes : terminer par une ligne vide (EntrÃ©e deux fois).
* Ajout dâ€™un script **`tools/benchmark_responses.py`** :

  * Choix du modÃ¨le IA au lancement (si DEV\_MODE activÃ©).
  * Choix du prompt Ã  envoyer.
  * Envoi automatique du mÃªme prompt plusieurs fois (benchmark).
  * Affichage du temps de rÃ©ponse par essai et calcul de stats (moyenne/min/max).
  * RÃ©sultats sauvegardÃ©s dans `data/dev_responses.*`.

### SÃ©paration UI / logique :

* Ajout dâ€™un client intermÃ©diaire `client/ia_client.py`.
* `main_ui.py` devient un simple lanceur.
* UI scindÃ©e en `ui/app_main.py` (logique) et `ui/layout_builder.py` (construction visuelle).
* `ui/` et `client/` sont des packages Python (`__init__.py`).

### UI / zone\_chat et zone\_message :

* Bulles adaptatives avec retour Ã  la ligne.
* Couleurs centralisÃ©es (`ui/config_ui.py`).
* Boutons copier sous chaque bulle (copie presse-papier + coche temporaire).
* CrÃ©ation de nouvelle conversation via bouton `+`.
* DÃ©tection et exÃ©cution des commandes spÃ©ciales `&msg1`, `&msg2`, `&run`.

### Centralisation des couleurs et icÃ´nes :

* `COLOR_USER_BUBBLE`, `COLOR_IA_BUBBLE`, etc.
* IcÃ´nes copier et coche avec effet hover.

### Sauvegarde / synthÃ¨se :

* Sauvegardes MD/TXT et extraction de code gÃ©rÃ©es par le backend.
* Commande spÃ©ciale de clÃ´ture (CLI) gÃ©nÃ¨re synthÃ¨se et README mis Ã  jour.

### Refactorisation sessions :

* Nouveau module `core/session_manager.py`.
* `commands.py` (CLI) et `IAClient` (UI) passent par ce module.
* `ChatManager` reste focalisÃ© sur la logique de chat.

---

## ğŸ“‚ Structure du projet (mise Ã  jour)

```
ğŸ“ IA_V3/
    ğŸ“„ arborescence.txt
    ğŸ“„ config.py
    ğŸ“„ debug.log
    ğŸ“„ main.py
    ğŸ“„ main_ui.py
    ğŸ“„ README.md
    ğŸ“ assets/
        ğŸ“ images/
            ğŸ“„ coche_icon.png
            ğŸ“„ copier_icon.png
            ğŸ“„ fond_window.png
            ğŸ“„ Logo_IA.png
            ğŸ“„ plus_icon.png
            ğŸ“„ send_icon.png
            ğŸ“„ send_icon2.png
    ğŸ“ client/
        ğŸ“„ ia_client.py
        ğŸ“„ __init__.py
    ğŸ“ core/
        ğŸ“„ chat_manager.py
        ğŸ“„ commands.py
        ğŸ“„ ollama_client.py
        ğŸ“„ sav_manager.py
        ğŸ“„ session_manager.py
        ğŸ“„ startup_utils.py
        ğŸ“„ __init__.py
        ğŸ“ logging/
            ğŸ“„ conv_logger.py
            ğŸ“„ logger.py
            ğŸ“„ __init__.py
    ğŸ“ data/
        ğŸ“„ dev_responses.jsonl
        ğŸ“„ dev_responses.log
        ğŸ“„ models_list.txt
    ğŸ“ logs/
        ğŸ“„ conversation.log
        ğŸ“„ sav_conv_*.log
        ğŸ“„ test11.log
        ğŸ“„ test12.log
    ğŸ“ sav/
        ğŸ“ sav_conv_*/
            ğŸ“„ conversation.md
        ğŸ“ test11/
            ğŸ“„ code_*.py
            ğŸ“„ conversation.md
        ğŸ“ test12/
            ğŸ“„ code_*.py
            ğŸ“„ conversation.md
    ğŸ“ tests/
        ğŸ“„ test_ui_load.py
        ğŸ“„ __init__.py
        ğŸ“ fenetre_kivy/
            ğŸ“„ __init__.py
    ğŸ“ tools/
        ğŸ“„ analyze_dev_logs.py
        ğŸ“„ benchmark_responses.py
        ğŸ“„ calc_fond_dims.py
        ğŸ“„ init_conv_chatgpt.py
        ğŸ“„ update_system_prompt.py
    ğŸ“ ui/
        ğŸ“„ app_main.py
        ğŸ“„ config_ui.py
        ğŸ“„ layout_builder.py
        ğŸ“„ __init__.py
        ğŸ“ behaviors/
            ğŸ“„ hover_behavior.py
            ğŸ“„ __init__.py
        ğŸ“ widgets/
            ğŸ“„ buttons.py
        ğŸ“ zones/
            ğŸ“„ zone_chat.py
            ğŸ“„ zone_liste_conv.kv
            ğŸ“„ zone_liste_conv.py
            ğŸ“„ zone_message.kv
            ğŸ“„ zone_message.py
            ğŸ“„ __init__.py
```

---

## ğŸ–¥ï¸ Commandes utiles Ollama (Windows / CMD)

* Lister les modÃ¨les installÃ©s :

```bash
ollama list
```

* TÃ©lÃ©charger un modÃ¨le :

```bash
ollama pull mistral
ollama pull deepseek-coder:33b
```

* Supprimer un modÃ¨le :

```bash
ollama rm mistral
ollama rm deepseek-coder:33b
```

* VÃ©rifier que le serveur Ollama tourne :

```bash
netstat -ano | findstr 11434
```

* Tester lâ€™API Ollama :

```bash
curl http://127.0.0.1:11434/api/tags
```

* DÃ©marrer manuellement le serveur (normalement inutile, Ollama tourne dÃ©jÃ  en service) :

```bash
ollama serve
```

---

## ğŸ“Š Analyse des performances

Un script dÃ©diÃ© (`tools/benchmark_responses.py`) permet :

* De choisir un modÃ¨le IA installÃ©.
* De choisir un prompt personnalisÃ©.
* De dÃ©finir un nombre dâ€™essais (par dÃ©faut 5).
* De mesurer le temps de rÃ©ponse de lâ€™IA pour chaque essai.
* De sauvegarder tous les rÃ©sultats (bruts + rÃ©sumÃ©) dans `data/`.

Les rÃ©sultats peuvent Ãªtre analysÃ©s avec **Pandas** en important `data/dev_responses.jsonl`.
