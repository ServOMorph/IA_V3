# IA\_V3 â€“ Chat IA avec Ollama, gestion avancÃ©e des conversations et interface Kivy

## ğŸ“Œ Description

Projet backend + UI pour interagir avec un modÃ¨le IA local via **Ollama**, avec gestion avancÃ©e des conversations, interface Kivy et **module de benchmark intÃ©grÃ©**.

### FonctionnalitÃ©s principales :

* Dialogue avec un modÃ¨le IA local.
* Sauvegarde des conversations dans un **dossier par session** (`/sav/<nom_session>/conversation.md`).
* Sauvegarde automatique des blocs de code Python extraits en `.py`.
* Chargement, renommage, suppression et organisation des sessions.
* Copie rapide des derniers messages dans le presse-papier (CLI).
* Conservation du contexte conversationnel.
* Logs techniques et conversationnels sÃ©parÃ©s (`/logs/<nom_session>.log`).
* **Interface Kivy moderne** avec zones distinctes : liste de conversations, chat, saisie, panneau info/config.
* Couleurs centralisÃ©es dans `ui/config_ui.py`.
* **Mode DEV** :

  * Liste les modÃ¨les Ollama installÃ©s au dÃ©marrage.
  * Permet de choisir le modÃ¨le IA Ã  utiliser.
  * Affiche et logge le temps de rÃ©ponse pour chaque prompt.
  * Sauvegarde chaque essai et benchmark dans `data/dev_responses.log` et `data/dev_responses.jsonl`.
* **Module Benchmark IA** :

  * ExÃ©cution de tests unitaires automatiques (ex. `is_prime`, `fibonacci`, `factorial`).
  * Scripts dÃ©diÃ©s pour lancer rapidement des benchmarks ciblÃ©s :

    * `benchmark_mistral.py` (Mistral standard)
    * `benchmark_mistral2.py` (Mistral optimisÃ© : `mistral-tests`)
    * `benchmark_deepseek.py` (DeepSeek optimisÃ© : `deepseek-tests`)
    * `benchmark_phi.py` (Phi-4 et Phi-4 Mini)
    * `benchmark_starling.py` (Starling-LM)
    * `benchmark_llava.py` (LLaVA multimodal)
  * Sauvegarde des rÃ©sultats (code gÃ©nÃ©rÃ© + stats) dans `data/generated/` et `data/dev_responses.*`.
  * Documentation associÃ©e dans `docs/` (prompt engineering et comparatifs).

---

## ğŸ“‚ Arborescence complÃ¨te

```
ğŸ“ IA_V3/
    ğŸ“„ arborescence.txt
    ğŸ“„ config.py
    ğŸ“„ debug.log
    ğŸ“„ main.py
    ğŸ“„ main_benchmark.py
    ğŸ“„ main_ui.py
    ğŸ“„ README.md
    ğŸ“ .pytest_cache/
    ğŸ“ .ruff_cache/
    ğŸ“ assets/
        ğŸ“ images/
            ğŸ“„ coche_icon.png
            ğŸ“„ copier_icon.png
            ğŸ“„ fond_window.png
            ğŸ“„ Logo_IA.png
            ğŸ“„ plus_icon.png
            ğŸ“„ send_icon.png
            ğŸ“„ send_icon2.png
    ğŸ“ client/
        ğŸ“„ ia_client.py
        ğŸ“„ __init__.py
    ğŸ“ core/
        ğŸ“„ chat_manager.py
        ğŸ“„ commands.py
        ğŸ“„ ollama_client.py
        ğŸ“„ sav_manager.py
        ğŸ“„ session_manager.py
        ğŸ“„ startup_utils.py
        ğŸ“„ __init__.py
        ğŸ“ logging/
            ğŸ“„ conv_logger.py
            ğŸ“„ logger.py
            ğŸ“„ __init__.py
    ğŸ“ data/
        ğŸ“„ dev_responses.jsonl
        ğŸ“„ dev_responses.log
        ğŸ“„ models_list.txt
        ğŸ“ generated/
            ğŸ“ deepseek-coder_6.7b/
            ğŸ“ deepseek-tests/
            ğŸ“ mistral-tests/
            ğŸ“ mistral_latest/
    ğŸ“ Docs/
        ğŸ“„ comparatif_mistral_vs_deepseek.md
        ğŸ“„ ollama_models_readme.md
        ğŸ“„ prompt_engineering_deepseek.md
        ğŸ“„ prompt_engineering_mistral7b.md
        ğŸ“„ prompt_engineering_phi.md
        ğŸ“„ prompt_engineering_starling.md
        ğŸ“„ prompt_engineering_llava.md
    ğŸ“ logs/
        ğŸ“„ conversation.log
        ğŸ“„ sav_conv_*.log
        ğŸ“„ test11.log
        ğŸ“„ test12.log
    ğŸ“ ollama_configs/
        ğŸ“„ README.md
        ğŸ“ deepseek_tests/
            ğŸ“„ Modelfile
            ğŸ“„ README.md
        ğŸ“ mistral_tests/
            ğŸ“„ Modelfile
            ğŸ“„ README.md
        ğŸ“ phi_tests/
            ğŸ“„ Modelfile
            ğŸ“„ README.md
        ğŸ“ starling_tests/
            ğŸ“„ Modelfile
            ğŸ“„ README.md
        ğŸ“ llava_tests/
            ğŸ“„ Modelfile
            ğŸ“„ README.md
    ğŸ“ sav/
        ğŸ“ sav_conv_*/
            ğŸ“„ conversation.md
        ğŸ“ test11/
            ğŸ“„ code_*.py
            ğŸ“„ conversation.md
        ğŸ“ test12/
            ğŸ“„ code_*.py
            ğŸ“„ conversation.md
    ğŸ“ tests/
        ğŸ“„ test_ui_load.py
        ğŸ“„ __init__.py
        ğŸ“ fenetre_kivy/
            ğŸ“„ __init__.py
    ğŸ“ tools/
        ğŸ“„ analyze_dev_logs.py
        ğŸ“„ benchmark_responses.py
        ğŸ“„ calc_fond_dims.py
        ğŸ“„ init_conv_chatgpt.py
        ğŸ“„ update_system_prompt.py
        ğŸ“ benchmark/
            ğŸ“„ benchmark_deepseek.py
            ğŸ“„ benchmark_mistral.py
            ğŸ“„ benchmark_phi.py
            ğŸ“„ benchmark_starling.py
            ğŸ“„ benchmark_llava.py
            ğŸ“„ cli.py
            ğŸ“„ code_utils.py
            ğŸ“„ config_benchmark.py
            ğŸ“„ exercises.py
            ğŸ“„ readme.md
            ğŸ“„ runner.py
            ğŸ“„ storage.py
            ğŸ“„ __init__.py
    ğŸ“ ui/
        ğŸ“„ app_main.py
        ğŸ“„ config_ui.py
        ğŸ“„ layout_builder.py
        ğŸ“„ __init__.py
        ğŸ“ behaviors/
            ğŸ“„ hover_behavior.py
            ğŸ“„ __init__.py
        ğŸ“ widgets/
            ğŸ“„ buttons.py
        ğŸ“ zones/
            ğŸ“„ zone_chat.py
            ğŸ“„ zone_liste_conv.kv
            ğŸ“„ zone_liste_conv.py
            ğŸ“„ zone_message.kv
            ğŸ“„ zone_message.py
            ğŸ“„ __init__.py
```

---

## ğŸ–¥ï¸ Commandes utiles Ollama (Windows / CMD)

* Lister les modÃ¨les installÃ©s :

```bash
ollama list
```

* TÃ©lÃ©charger un modÃ¨le :

```bash
ollama pull mistral:7b
ollama pull deepseek-coder:6.7b
ollama pull phi4:latest
ollama pull phi4-mini:latest
ollama pull starling-lm:7b
ollama pull llava:7b
```

* Supprimer un modÃ¨le :

```bash
ollama rm mistral:7b
ollama rm phi4-mini:latest
```

* VÃ©rifier que le serveur Ollama tourne :

```bash
curl http://127.0.0.1:11434/api/tags
```

* DÃ©marrer manuellement le serveur :

```bash
ollama serve
```

---

## ğŸ“Š Analyse des performances

* Benchmarks automatisÃ©s via `tools/benchmark/`.
* RÃ©sultats sauvegardÃ©s dans `data/dev_responses.*`.
* Analyse possible via :

```bash
python tools/analyze_results.py
```

* Comparatifs disponibles :

  * `docs/comparatif_mistral_vs_deepseek.md`
  * `docs/prompt_engineering_phi.md`
  * `docs/prompt_engineering_starling.md`
  * `docs/prompt_engineering_llava.md`

---

## âš¡ Conseils dâ€™optimisation des performances

* **Quantisation** : utiliser les variantes Q4/Q5 (`-q4_K_M`) pour rÃ©duire la VRAM et accÃ©lÃ©rer lâ€™infÃ©rence.
* **Context length** : rester Ã  4k ou 8k tokens pour garder lâ€™exÃ©cution GPU. Au-delÃ , bascule CPU â†’ ralentissements.
* **Choix des modÃ¨les adaptÃ©s Ã  la RTX 4060 (8 Go VRAM)** :

  * GÃ©nÃ©ral : `mistral:7b`, `llama3.1:8b-q4`, `gemma:7b`, `phi4-mini:3.8b`
  * Code : `deepseek-coder:6.7b`, `qwen2.5-coder:7b`
  * Multimodal : `llava:7b`
  * Raisonnement : `starling-lm:7b`, `phi4:latest`
* **Batch size** : rÃ©duire si la VRAM est saturÃ©e.
* **Threads CPU** : utiliser 16 threads sur Ryzen 7 5700X pour compenser en mode CPU fallback.

---

## ğŸŒŸ ModÃ¨les recommandÃ©s (Top 5)

* **Mistral 7B** â†’ usage gÃ©nÃ©ral rapide, polyvalent.
* **Qwen2.5-Coder 7B** â†’ spÃ©cialisÃ© code.
* **LLaVA 7B Q4** â†’ multimodal texte+image.
* **Starling-LM 7B** â†’ raisonnement/dÃ©bat.
* **Phi-4 Mini 3.8B** â†’ compact, fluide pour tÃ¢ches rapides.

---

## ğŸ”® AmÃ©liorations prÃ©vues

* Nouveaux benchmarks pour Phi-4, Starling-LM et LLaVA.
* Automatisation des tests multimodaux (texte + image).
* Export automatique en CSV/Excel.
* Dashboard de visualisation (Grafana/Streamlit).
* Optimisation GPU/paramÃ¨tres supplÃ©mentaires pour les modÃ¨les lourds.
* IntÃ©gration dâ€™un gestionnaire de profils (configurations par modÃ¨le).
