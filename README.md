# IA\_V3 ‚Äì Chat IA avec Ollama, gestion avanc√©e des conversations et interface Kivy

## üìå Description

Projet backend + UI pour interagir avec un mod√®le IA local via **Ollama**, avec gestion avanc√©e des conversations, interface Kivy et **module de benchmark int√©gr√©**.

### Fonctionnalit√©s principales :

* Dialogue avec un mod√®le IA local.
* Sauvegarde des conversations dans un **dossier par session** (`/sav/<nom_session>/conversation.md`).
* Sauvegarde automatique des blocs de code Python extraits en `.py`.
* Chargement, renommage, suppression et organisation des sessions.
* Copie rapide des derniers messages dans le presse-papier (CLI).
* Conservation du contexte conversationnel avec possibilit√© de tronquer l'historique pour acc√©l√©rer les r√©ponses.
* **Syst√®me de r√©sum√© avanc√©** :

  * R√©sum√© **glissant** par tranches de messages.
  * R√©sum√©s **partiels num√©rot√©s** conserv√©s dans `summary.md`.
  * R√©sum√©s **globaux p√©riodiques** consolidant les partiels.
  * Format structur√© en 4 sections : Faits cl√©s, Intentions utilisateur, R√©ponses IA, Points en suspens.
* Logs techniques et conversationnels s√©par√©s (`/logs/<nom_session>.log`).
* **Interface Kivy moderne** avec zones distinctes : liste de conversations, chat, saisie, panneau info/config.
* Couleurs centralis√©es dans `ui/config_ui.py`.
* **Mode DEV** :

  * Liste les mod√®les Ollama install√©s au d√©marrage.
  * Permet de choisir le mod√®le IA √† utiliser.
  * Affiche et logge le temps de r√©ponse pour chaque prompt.
  * Sauvegarde chaque essai et benchmark dans `data/dev_responses.log` et `data/dev_responses.jsonl`.
* **Module Benchmark IA** :

  * Ex√©cution de tests unitaires automatiques (ex. `is_prime`, `fibonacci`, `factorial`).
  * Benchmarks multi-mod√®les et multi-param√®tres (`MAX_TOKENS`, quantisation, etc.).
  * Scripts d√©di√©s pour lancer rapidement des benchmarks cibl√©s :

    * `benchmark_mistral.py` (Mistral standard)
    * `benchmark_mistral2.py` (Mistral optimis√© : `mistral-tests`)
    * `benchmark_deepseek.py` (DeepSeek optimis√© : `deepseek-tests`)
    * `benchmark_phi.py` (Phi-4 et Phi-4 Mini)
    * `benchmark_starling.py` (Starling-LM)
    * `benchmark_llava.py` (LLaVA multimodal)
    * `benchmark_suite.py` et `benchmark_tokens.py` pour tests comparatifs avanc√©s
  * Sauvegarde des r√©sultats (code g√©n√©r√© + stats) dans `data/generated/` et `data/dev_responses.*`.
  * Documentation associ√©e dans `docs/` (prompt engineering et comparatifs).

## üñ•Ô∏è Commandes utiles Ollama (Windows / CMD)

* Lister les mod√®les install√©s :

```bash
ollama list
```

* T√©l√©charger un mod√®le :

```bash
ollama pull mistral:7b
```

* Supprimer un mod√®le :

```bash
ollama rm mistral:7b
```

* V√©rifier que le serveur Ollama tourne :

```bash
curl http://127.0.0.1:11434/api/tags
```

* D√©marrer manuellement le serveur :

```bash
ollama serve
```

---

## üìä Analyse des performances

* Benchmarks automatis√©s via `tools/benchmark/` et `perf_tests/`.
* R√©sultats sauvegard√©s dans `data/dev_responses.*`.
* Analyse possible via :

```bash
python tools/analyze_results.py
```

* Comparatifs disponibles :

  * `docs/comparatif_mistral_vs_deepseek.md`
  * `docs/prompt_engineering_phi.md`
  * `docs/prompt_engineering_starling.md`
  * `docs/prompt_engineering_llava.md`

---

## ‚ö° Conseils d‚Äôoptimisation des performances

* **Quantisation** : utiliser les variantes Q4/Q5 (`-q4_K_M`) pour r√©duire la VRAM et acc√©l√©rer l‚Äôinf√©rence. Exemple : `gemma2-2b-it-q4`.
* **Limite de g√©n√©ration** : d√©finir `MAX_TOKENS` dans `config.py` (ex. 100, 200, 400) pour contr√¥ler la longueur des r√©ponses et r√©duire la latence.
* **Troncature de l‚Äôhistorique** : limiter le nombre d‚Äô√©changes conserv√©s dans la m√©moire du client pour √©viter un contexte trop long.
* **Context length** : rester √† 4k ou 8k tokens pour garder l‚Äôex√©cution GPU. Au-del√†, bascule CPU ‚Üí ralentissements.
* **Choix des mod√®les adapt√©s √† la RTX 4060 (8 Go VRAM)** :

  * G√©n√©ral : `gemma2:2b`, `gemma2-2b-it-q4`, `mistral:7b`, `llama3.1:8b-q4`, `phi4-mini:3.8b`
  * Code : `deepseek-coder:6.7b`, `qwen2.5-coder:7b`
  * Multimodal : `llava:7b`
  * Raisonnement : `starling-lm:7b`, `phi4:latest`
* **Batch size** : r√©duire si la VRAM est satur√©e.
* **Threads CPU** : utiliser 16 threads sur Ryzen 7 5700X pour compenser en mode CPU fallback.
* **Pr√©chargement** : lancer `ollama serve` pour √©viter les temps de rechargement du mod√®le √† chaque requ√™te.

---

## üîÆ Am√©liorations pr√©vues

* Nouveaux benchmarks pour Phi-4, Starling-LM et LLaVA.
* Automatisation des tests multimodaux (texte + image).
* Export automatique en CSV/Excel.
* Dashboard de visualisation (Grafana/Streamlit).
* Optimisation GPU/param√®tres suppl√©mentaires pour les mod√®les lourds.
* Int√©gration d‚Äôun gestionnaire de profils (configurations par mod√®le).
* Ajout d‚Äôun export JSON structur√© des r√©sum√©s (pour m√©moire s√©lective et r√©injection cibl√©e).
* Commandes utilisateur pour contr√¥ler les r√©sum√©s (`/resumeshow`, `/resumerefresh`, `/resumeclear`).

12/09/2025 22:12
