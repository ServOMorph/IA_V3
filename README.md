# IA\_V3 ‚Äì Chat IA avec Ollama, gestion avanc√©e des conversations et interface Kivy

## üìå Description

Projet backend + UI pour interagir avec un mod√®le IA local via **Ollama**, avec gestion avanc√©e des conversations, interface Kivy et **module de benchmark int√©gr√©**.

### Fonctionnalit√©s principales :

* Dialogue avec un mod√®le IA local.
* Sauvegarde des conversations dans un **dossier par session** (`/sav/<nom_session>/conversation.md`).
* Sauvegarde automatique des blocs de code Python extraits en `.py`.
* Chargement, renommage, suppression et organisation des sessions.
* Copie rapide des derniers messages dans le presse-papier (CLI).
* Conservation du contexte conversationnel.
* Logs techniques et conversationnels s√©par√©s (`/logs/<nom_session>.log`).
* **Interface Kivy moderne** avec zones distinctes : liste de conversations, chat, saisie, panneau info/config.
* Couleurs centralis√©es dans `ui/config_ui.py`.
* **Mode DEV** :

  * Liste les mod√®les Ollama install√©s au d√©marrage.
  * Permet de choisir le mod√®le IA √† utiliser.
  * Affiche et logge le temps de r√©ponse pour chaque prompt.
  * Sauvegarde chaque essai et benchmark dans `data/dev_responses.log` et `data/dev_responses.jsonl`.
* **Module Benchmark IA** :

  * Ex√©cution de tests unitaires automatiques (ex. `is_prime`, `fibonacci`, `factorial`).
  * Scripts d√©di√©s pour lancer rapidement des benchmarks cibl√©s :

    * `benchmark_mistral.py` (Mistral standard)
    * `benchmark_mistral2.py` (Mistral optimis√© : `mistral-tests`)
    * `benchmark_deepseek.py` (DeepSeek optimis√© : `deepseek-tests`)
    * `benchmark_phi.py` (Phi-4 et Phi-4 Mini)
    * `benchmark_starling.py` (Starling-LM)
    * `benchmark_llava.py` (LLaVA multimodal)
  * Sauvegarde des r√©sultats (code g√©n√©r√© + stats) dans `data/generated/` et `data/dev_responses.*`.
  * Documentation associ√©e dans `docs/` (prompt engineering et comparatifs).


## üñ•Ô∏è Commandes utiles Ollama (Windows / CMD)

* Lister les mod√®les install√©s :

```bash
ollama list
```

* T√©l√©charger un mod√®le :

```bash
ollama pull mistral:7b
```

* Supprimer un mod√®le :

```bash
ollama rm mistral:7b
```

* V√©rifier que le serveur Ollama tourne :

```bash
curl http://127.0.0.1:11434/api/tags
```

* D√©marrer manuellement le serveur :

```bash
ollama serve
```

---

## üìä Analyse des performances

* Benchmarks automatis√©s via `tools/benchmark/`.
* R√©sultats sauvegard√©s dans `data/dev_responses.*`.
* Analyse possible via :

```bash
python tools/analyze_results.py
```

* Comparatifs disponibles :

  * `docs/comparatif_mistral_vs_deepseek.md`
  * `docs/prompt_engineering_phi.md`
  * `docs/prompt_engineering_starling.md`
  * `docs/prompt_engineering_llava.md`

---

## ‚ö° Conseils d‚Äôoptimisation des performances

* **Quantisation** : utiliser les variantes Q4/Q5 (`-q4_K_M`) pour r√©duire la VRAM et acc√©l√©rer l‚Äôinf√©rence.
* **Context length** : rester √† 4k ou 8k tokens pour garder l‚Äôex√©cution GPU. Au-del√†, bascule CPU ‚Üí ralentissements.
* **Choix des mod√®les adapt√©s √† la RTX 4060 (8 Go VRAM)** :

  * G√©n√©ral : `mistral:7b`, `llama3.1:8b-q4`, `gemma:7b`, `phi4-mini:3.8b`
  * Code : `deepseek-coder:6.7b`, `qwen2.5-coder:7b`
  * Multimodal : `llava:7b`
  * Raisonnement : `starling-lm:7b`, `phi4:latest`
* **Batch size** : r√©duire si la VRAM est satur√©e.
* **Threads CPU** : utiliser 16 threads sur Ryzen 7 5700X pour compenser en mode CPU fallback.


## üîÆ Am√©liorations pr√©vues

* Nouveaux benchmarks pour Phi-4, Starling-LM et LLaVA.
* Automatisation des tests multimodaux (texte + image).
* Export automatique en CSV/Excel.
* Dashboard de visualisation (Grafana/Streamlit).
* Optimisation GPU/param√®tres suppl√©mentaires pour les mod√®les lourds.
* Int√©gration d‚Äôun gestionnaire de profils (configurations par mod√®le).
